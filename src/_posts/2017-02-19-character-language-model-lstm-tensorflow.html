---
layout: post
title: "An Interactive Character-Level Language Model"
disqus_id: "char-lm-1"
pinned: true
github_url: https://github.com/batzner/tensorlm
category: "general"
html_title: "An Interactive Character-Level Language Model built with LSTMs in TensorFlow"
meta_description: "I let an LSTM read texts one character at a time. Find out, what it learned, by feeding it some letters in this interactive post."
focus_keyword: "Language Model"
summary: "I let a neural network read long texts one letter at a time. Its task was to predict the next letter based on those it had seen so far. Over time, it recognized patterns between letters. Find out, what it learned, by feeding it some letters."
excerpt: "<p>I let a neural network read long texts one letter at a time. Its task was to predict the next letter based on those it had seen so far. Over time, it recognized patterns between letters. Find out, what it learned, by feeding it some letters below. When you click the send button on the right, it will read your text and auto-complete it.</p>
<p>You can choose between networks that read a lot of Wikipedia articles, US Congress transcripts etc.</p>
<a href=\"/post/character-language-model-lstm-tensorflow\" style=\"color:white\"><div class=\"talk-box\" >
    <table>
        <tr class=\"talk-box-heading\">
            <td></td>
            <td class=\"no-stretch\">
                <div>Generate text from</div>
                <div class=\"btn-group\" role=\"group\" aria-label=\"Dataset choice\">
                    <button type=\"button\" value=\"congress\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">US Congress</button>
                    <button type=\"button\" value=\"wiki\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">Wikipedia</button>
                    <button type=\"button\" value=\"sherlock\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">Sherlock Holmes</button>
                    <button type=\"button\" value=\"southPark\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">South Park</button>
                    <button type=\"button\" value=\"goethe\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">Goethe</button>
                </div>
            </td>
            <td></td>
        </tr>
        <tr class=\"talk-box-body\">
            <td></td>
            <td>
                <div class=\"talk-box-input\" onclick=\"focusOnInput()\">
                    <div class=\"text-input\" contentEditable=\"true\" oninput=\"onInput()\"></div>
                    <div class=\"text-input-output-bridge\">...</div>
                    <span class=\"text-output\"></span>
                </div>
            </td>
            <td>
                <button type=\"button\" class=\"send-button btn btn-default round btn-icon\" onclick=\"completeText()\">
                    <span class=\"fa fa-paper-plane\"></span>
                </button>
            </td>
        </tr>
    </table>
</div>

<div class=\"talk-box mobile\">
    <div class=\"talk-box-heading\">
        <div>Generate text from</div>
        <div id=\"talk-box-dataset-dropdown\" class=\"btn-group dropdown-full-width\">
            <button type=\"button\" value=\"wiki\" class=\"btn btn-default dropdown-toggle dark\" onchange=\"selectDataset(null)\" data-toggle=\"dropdown\" aria-haspopup=\"true\" aria-expanded=\"false\">
                <span class=\"choice\">Wikipedia</span> <span class=\"caret\"></span>
            </button>
            <ul class=\"dropdown-menu dark\">
                <li><a href=\"#\" data-value=\"congress\">US Congress</a></li>
                <li><a href=\"#\" data-value=\"wiki\">Wikipedia</a></li>
                <li><a href=\"#\" data-value=\"sherlock\">Sherlock Holmes</a></li>
                <li><a href=\"#\" data-value=\"southPark\">South Park</a></li>
                <li><a href=\"#\" data-value=\"goethe\">Goethe Poems</a></li>
            </ul>
        </div>
    </div>
    <div class=\"talk-box-body\">
        <div class=\"talk-box-input\" onclick=\"focusOnInput()\">
            <div class=\"text-input\" contentEditable=\"true\" oninput=\"onInput()\"></div>
            <div class=\"text-input-output-bridge\">...</div>
            <span class=\"text-output\"></span>
        </div>
        <button type=\"button\" class=\"send-button btn btn-default dark\" onclick=\"completeText()\">
            Send
        </button>
    </div>
</div></a>"
tags: [neural-networks, lstm, nlp]
stylesheets: "<link rel=\"stylesheet\" href=\"/css/posts/char-lm.css\">"
javascripts: "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/he/1.1.1/he.min.js\"></script>
<script src=\"https://cdn.rawgit.com/nnattawat/flip/master/dist/jquery.flip.min.js\"></script>
<script src=\"/js/posts/char-lm/text-complete.js\"></script>
<script src=\"/js/posts/char-lm/plot-flexible.js\"></script>
<script src=\"/js/posts/char-lm/plot.js\"></script>
<script src=\"/js/posts/char-lm/plot-datasets.js\"></script>"
---
<p>I let a neural network read long texts one letter at a time. Its task was to predict the next letter based on those it had seen so far. Over time, it recognized patterns between letters. Find out, what it learned, by feeding it some letters below. When you click the send button on the right, it will read your text and auto-complete it.</p>
<p>You can choose between networks that read a lot of Wikipedia articles, US Congress transcripts etc.</p>

<div class="talk-box">
    <table>
        <tr class="talk-box-heading">
            <td></td>
            <td class="no-stretch">
                <div>Generate text from</div>
                <div class="btn-group" role="group" aria-label="Dataset choice">
                    <button type="button" value="congress" onclick="selectDataset(this.value)" class="btn btn-default dark">US Congress</button>
                    <button type="button" value="wiki" onclick="selectDataset(this.value)" class="btn btn-default dark">Wikipedia</button>
                    <button type="button" value="sherlock" onclick="selectDataset(this.value)" class="btn btn-default dark">Sherlock Holmes</button>
                    <button type="button" value="southPark" onclick="selectDataset(this.value)" class="btn btn-default dark">South Park</button>
                    <button type="button" value="goethe" onclick="selectDataset(this.value)" class="btn btn-default dark">Goethe</button>
                </div>
            </td>
            <td></td>
        </tr>
        <tr class="talk-box-body">
            <td></td>
            <td>
                <div class="talk-box-input" onclick="focusOnInput()">
                    <div class="text-input" contentEditable="true" oninput="onInput()"></div>
                    <div class="text-input-output-bridge">...</div>
                    <span class="text-output"></span>
                </div>
            </td>
            <td>
                <button type="button" class="send-button btn btn-default round btn-icon" onclick="completeText()">
                    <span class="fa fa-paper-plane"></span>
                </button>
            </td>
        </tr>
    </table>
</div>

<div class="talk-box mobile">
    <div class="talk-box-heading">
        <div>Generate text from</div>
        <div id="talk-box-dataset-dropdown" class="btn-group dropdown-full-width">
            <button type="button" value="wiki" class="btn btn-default dropdown-toggle dark" onchange="selectDataset(null)" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <span class="choice">Wikipedia</span> <span class="caret"></span>
            </button>
            <ul class="dropdown-menu dark">
                <li><a href="#\" data-value="congress">US Congress</a></li>
                <li><a href="#\" data-value="wiki">Wikipedia</a></li>
                <li><a href="#\" data-value="sherlock">Sherlock Holmes</a></li>
                <li><a href="#\" data-value="southPark">South Park</a></li>
                <li><a href="#\" data-value="goethe">Goethe Poems</a></li>
            </ul>
        </div>
    </div>
    <div class="talk-box-body">
        <div class="talk-box-input" onclick="focusOnInput()">
            <div class="text-input" contentEditable="true" oninput="onInput()"></div>
            <div class="text-input-output-bridge">...</div>
            <span class="text-output"></span>
        </div>
        <button type="button" class="send-button btn btn-default dark" onclick="completeText()">
            Send
        </button>
    </div>
</div>

<p>Here is the detailed description of what I did: I used a specific form of recurrent neural networks, the <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">LSTM (Long Short-Term Memory)</a>, to learn a language model for a given text corpus. Because I fed it only one letter at a time, it learned a language model on a character level. This idea is not new at all. I was inspired by <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">this popular blog post</a> by Andrej Karpathy. He also trained character-level networks on Shakespeare, Wikipedia, Linux Source Code etc. The results are amazing.</p>

<p>I made my code available via <a href="https://github.com/batzner/tensorlm">GitHub</a> and a <a href="https://pypi.python.org/pypi/tensorlm">Python package</a>. The final models that power the text box above run on an AWS instance. As it turns out, five TensorFlow models with 8 to 13 million parameters each can run simultaneously on a single t2.micro instance with only 1 GiB of RAM.</p>

<p>I did not do a lot of hyper-parameter tuning and the tuning I did yielded marginal results. Below are the hyper-parameters that I found to work well for each dataset. For all models, I used 90-dimensional one-hot encodings as input and output of the model. The models were trained by minimizing the cross-entropy / bits per character using <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop" target="_blank">RMSprop</a>.</p>

<h3>Model</h3>
<ul>
    <li>Number of LSTM layers: 3</li>
    <li>Number of neurons per layer: 795</li>
    <li>Batch size: 100</li>
    <li>Learning rate: 0.001</li>
    <li>Dropout: 0.5</li>
    <li>Gradient norm bound: 5</li>
    <li>Weight initialization: <a href="https://www.tensorflow.org/api_docs/python/tf/glorot_normal_initializer">Xavier</a></li>
</ul>
<p>Small details:</p>
<ul>
    <li>A dense softmax layer with 90 units followed after the last LSTM layer.</li>
    <li>The South Park and the Goethe model had two instead of three LSTM layers.</li>
    <li>I reset the LSTM's hidden state every 3000 time steps. This leads to a stateful LSTM that treats the initial zero hidden state as a true zero state. I found that never resetting the hidden state allows the model to "store" information in variables being zero in the hidden state. For example, it often closed a bracket it never opened when completing a sentence.</li>
    <li>I trained all models on an AWS p2.xlarge instance ($0.2 per hour).</li>
</ul>
<p>A dense softmax layer with 90 units followed after the last LSTM layer.  </p>

<h3>Datasets</h3>
<strong>US Congress</strong>
<ul>
    <li>488 million characters from transcripts of the United States Senate's congressional record</li>
    <li>Trained for 2 days, but already converged after about 21 hours / 3 epochs</li>
</ul>
<strong>Wikipedia</strong>
<ul>
    <li>447 million characters from about 140,000 articles (4.6% of the English Wikipedia)</li>
    <li>Trained for 2 days, but already converged after about 20 hours / 3 epochs</li>
</ul>
<strong>Sherlock</strong>
<ul>
    <li>3.6 million characters (about 650,000 words) from the whole <a href="https://sherlock-holm.es/stories/plain-text/cano.txt" target="_blank">Sherlock Holmes corpus</a> by Sir Arthur Conan Doyle. I removed indentation but kept all line breaks even if their only purpose was formatting.</li>
    <li>Trained for 3 hours.</li>
</ul>
<strong>South Park</strong>
<ul>
    <li>4.7 million characters from all 277 South Park episodes</li>
    <li>Used 2 layers with 795 neurons to prevent overfitting</li>
    <li>Trained for 2 hours.</li>
</ul>
<strong>Goethe</strong>
<ul>
    <li>1.5 million characters from all poems by Johann Wolfgang von Goethe</li>
    <li>I used 2 layers with 795 neurons to prevent overfitting. With 3 layers, the network completely remembered whole poems.</li>
    <li>Trained for 2 hours.</li>
</ul>
<p>As training / validation split, I used a 90 / 10 ratio for the three small datasets and a 95 / 5 ratio for the US Congress and the Wikipedia dataset.</p>

<p>Throughout the training of all experiments, I let the models complete a sentence started with "The " every now and then. This was a good evaluation metric in addition to the validation loss. Here is the output of the model with a batch size of 200 after training:</p>

<blockquote>The house which had been the strange problem which had been the strange of the man who had been the strange of the man who had been the strange of the man who had been the strange of the man who had been the strange of the man who had been the stran...</blockquote>

<blockquote>The hand of the man who had been the same as the other. The man was still sent to him that he had not been...</blockquote>
<blockquote>The house of Dr. Mortimer, who was a man of excellent character, which was the most profound feeling of...</blockquote>

<p>Which one sounds more like an actual Sherlock Holmes excerpt? In my opinion, the second one because it uses an actual name instead of just talking about a man and another man. Both sentences were sampled during the training of the model with 1024 neurons per layer. But while the second one was sampled at the end with a validation perplexity of 2.47, the first sentence was generated when the model reached its lowest validation perplexity of 2.26. So the more realistic sounding model is actually more confused when we reading unseen Sherlock Holmes text. It assigns a lower probability to it, which means it is a worse language model.</p>

<p>I think it is reasonable that the better non-overfitting model generates such generic sentences. Say that for each noun describing a person in the Sherlock Holmes corpus there is a 20% probability of it being "man", 20% for "woman", 10% for "Sherlock Holmes", 5% for "Mrs. Watson", 5% for "Dr. Mortimer", 5% for "Watson", 5% for "Dr. Moriarty and so on. Now, if a human were given the task to generate a sentence he/she would use names because there is a 60% probability of the noun being a name and a 40% probability of the noun being "man" or "woman". But during training, we don't let the model generate sentences. We only ask it for the next character/word. So, if the model knows that the next word will be a noun describing a person it has the choice between multiple different nouns. Each name of these nouns might have a 5% probability, but "man" has 20%, so it goes for "man".</p>
<p>While this behavior is understandable, it is not optimal. An optimal model would have learned that while the probability of "man" is high, the probability of a sentence using only "man" to describe what persons are doing is low. But until our model gets so smart, we have to forgive generic sentences and resists the temptation of overfitting.</p>


<h3>Resetting the LSTM state</h3>
<p>There is one last experiment that I did, which was related to resetting the LSTM's internal hidden state. So far the internal state never got reset during training. It worked fine for the Sherlock dataset, but when training on Wikipedia articles, the model always generated something like this:</p>
<blockquote>The Victorian Artist of the Year 1943) was a student of the University of California...</blockquote>
<blockquote>The Victorian Army and the United States Congress of the United States and the United States) and the Committee of the American...</blockquote>

<p>Sounds good except for that closing bracket in every sentence. I assume that happens because the model only sees a zero hidden state once during training - at the very beginning. After that, the state never gets reset to a zero vector. Thus, the model can safely store information in the hidden state and even attribute information to a zero hidden state, such as "I need to close the bracket soon". For validation and sampling, however, the model starts again with a zero state, so it closes the bracket that was never opened.</p>
<p>Resetting the hidden state to zero every now and then solves this problem. I tested different token intervals on Sherlock. "always" means that the state got reset after each backpropagation run (160 tokens). Similarly, 320 means that the state got reset after 320 tokens per batch / 2 backpropagation runs.</p>

<div class="content-9">
    <div id="dataset-card">
        <div class="front">
            <canvas id="dataset-chart" class="chart" data-width="600" data-height="300"></canvas>
        </div>
        <div class="back">
            <blockquote id="dataset-excerpt"></blockquote>
        </div>
    </div>
</div>
<div class="content-3">
    <div class="chart-form-group">
        <div>
            <label for="dataset-dropdown">Dataset</label>
            <div id="dataset-dropdown" class="btn-group dropdown-full-width">
                <button type="button" value="southPark" class="btn btn-default dropdown-toggle" onchange="onDatasetChanged()" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    <span class="choice">South Park</span> <span class="caret"></span>
                </button>
                <ul class="dropdown-menu">
                    <li><a href="#\" data-value="wiki">Wikipedia</a></li>
                    <li><a href="#\" data-value="congress">Congress Transcripts</a></li>
                    <li><a href="#\" data-value="goethe">Goethe Poems</a></li>
                    <li><a href="#\" data-value="southPark">South Park</a></li>
                    <li><a href="#\" data-value="sherlock">Sherlock</a></li>
                </ul>
            </div>
        </div>
        <div>
            <button type="button" value="showExcerpt" id="flip-dataset-card-button" class="btn btn-default" onclick="flipDatasetCard()">Show Excerpt</button>
        </div>
    </div>
</div>
<div class="clearfix"></div>

<p>I also fed every validation dataset to each of the models and measured their average perplexity. Here is how confused they were:</p>

<table id="perplexity-table" data-transform="color-cells-by-log-value">
    <tr>
        <td></td>
        <td colspan="5" class="table-heading-columns">Trained on</td>
    </tr>
    <tr>
        <td>Evaluated on</td>
        <th>Sherlock</th>
        <th>Wikipedia</th>
        <th>Congress</th>
        <th>South Park</th>
        <th>Goethe</th>
    </tr>
    <tr>
        <th>Sherlock</th>
        <td>2.11</td>
        <td>4.32</td>
        <td>3.14</td>
        <td>3.56</td>
        <td>37.01</td>
    </tr>
    <tr>
        <th>Wikipedia</th>
        <td>2.77</td>
        <td>2.22</td>
        <td>3.20</td>
        <td>3.97</td>
        <td>37.53</td>
    </tr>
    <tr>
        <th>Congress</th>
        <td>4.00</td>
        <td>2.51</td>
        <td>1.73</td>
        <td>4.11</td>
        <td>48.50</td>
    </tr>
    <tr>
        <th>South Park</th>
        <td>3.89</td>
        <td>3.01</td>
        <td>3.48</td>
        <td>2.31</td>
        <td>40.22</td>
    </tr>
    <tr>
        <th>Goethe</th>
        <td>18.38</td>
        <td>6.36</td>
        <td>11.88</td>
        <td>14.93</td>
        <td>3.20</td>
    </tr>
</table>

<p>We can see that Goethe was quite confused by the English language. The Wikipedia model seems to generalize best. But considering its low perplexity even on the Goethe dataset, I think that the diversity of the Wikipedia dataset also cause the model to be generally unsure in its predictions. Thus, it is less confused, if it was wrong. Similarly, the strong homogeneity of Congress transcripts facilitates scoring a low perplexity.</p>