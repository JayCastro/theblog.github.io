---
layout: post
title: "Yet another Derivation of the Variational Autoencoder"
disqus_id: "vae-derivation"
category: "general"
html_title: "Yet another Derivation of the Variational Autoencoder"
meta_description: ""
focus_keyword: "VAE"
summary: ""
stylesheets: ""
javascripts: ""
---

<p><i>"How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?"</i> - the Auto-Encoding Variational Bayes paper by Kingma and Welling [1] starts off with several terms that require knowledge of probabilistic models. This can be a challenge to people coming from a Deep Learning background. Luckily, there are several good blog posts [TODO] that introduce Variational Autoencoders in a simplified manner. After reading these, I still had some open questions regarding small details that these posts omitted. So, in this blog post I provide <i>yet another</i> derivation of the Variational Autoencoder aimed at readers with open questions. I will start at zero and define the terms used by Kingma and Welling [1] step by step.</p>

<h3>\(p(x|X)\) is interesting</h3>

<p>We have a dataset, \(X\) consisting of \(N\) examples \(x_1, ..., x_n\). Given \(X\), it would be interesting to know \(p(x|X)\). We could use it to predict if a new \(x\) belongs to the dataset or not. This is helpful for clustering, anomaly detection, data generation and <a href="https://ermongroup.github.io/cs228-notes/preliminaries/applications/">other unsupervised learning tasks.</a></p>

<p>For modeling \(p(x|X)\), we can try to approximate it with a distribution \(p_\theta(x) \approx p(x|X)\). This distribution \(p_\theta(x)\) is parametrized by \(\theta\). \(\theta\) could for example be the parameters of a neural network, which outputs \(p_\theta(x)\), i.e. the prediction for the real and unknown \(p(x|X)\). <!--To find a good approximation of \(p(x|X)\), we could train the neural network using \(X\) and Maximum Likelihood Estimation.--></p>

<h3>\(p(x|X)\) is hard to model</h3>

<p>The problem with \(p(x|X)\) is that it is close to zero almost everywhere for many interesting datasets. For example, the CIFAR-10 dataset contains 60,000 32x32 images, i.e. points in a 1024-dimensional space, the pixel space. Randomly generating an image in this space will likely not produce an image of an object from one of the 10 classes. Following the manifold hypothesis, we can therefore assume that the data points lie on a lower-dimensional manifold. This means that \(p(x|X) \approx 0\) for \(x\) outside of this manifold. Thus, \(p(x|X)\) has to be very large for \(x\) on the manifold, since \( \int p(x|X) \,d x \overset{!}{=} 1\). Such a function is very hard to model with a neural network.</p>

<h3>\(p(x|X,z)\) is easier to model</h3>

<p>We can make modeling \(p(x|X)\) easier by incorporating a prior assumption into the model architecture. The assumption is that the examples \(x\) depend on latent / hidden variables \(z\). This means that there are hidden causes behind the examples in our dataset. This is a natural assumption for many datasets. For example, the latent variables behind images of faces include azimuth, skin color, lighting etc.</p>

<p>If we know the exact values of \(z\) for every \(x\), modeling \(p(x|X, z)\) will likely be easier than modeling \(p(x|X)\) directly. This is evident for simple datasets, where a few latent variables can completely explain every sample. For example, in the figure above, the elevation and azimuth variables completely determine every image. If we can compute the image \(g\) from these two variables, we can directly model \(p(x|X, z)\) - simply by checking if \(x = g\). But even if there is uncertainty left with respect to \(x\), \(p(x|X, z)\) will likely have fewer regions of high probabiliy mass. Thus, modeling \(p(x|X, z)\) will be easier than modeling \(p(x|X)\) directly. Given that we also know the probability distribution \(p(z)\) of the latent variables themselves, we can now model \(p(x|X)\) via</p>

$$p_\theta(x) = \int_z p_\theta(x|z) p_\theta(z) \,dz$$

<p>Here, \(\theta\) consists of all parameters needed to model \(p_(x|X,z)\) and \(p_(z)\). It indirectly also parametrizes  p_\theta(x). In conclusion, knowing the exact values of some latent variables \(z\) for a given \(x\) will probably make modeling \(p(x|X,)\) easier.</p>

<h3>TODO</h3>

<p>The problem is that we don't know the values of the latent variables. We don't even know what and how many latent variables there are. That is what makes them latent.

<h3>Notes</h3>
<ul>
    <li>The notation used in this post follows the notation of "Deep Learning" by Goodfellow et al. [TODO]</li>
</ul>

------

TODO:
- Neuronale Netze getrennt einführen! Vorher alles NN agnostisch machen.
- die terme des VAE papers (oder beider VAE paper) der reihe nach einführen und jedes mal eine bootstrap alert box hinzufügen, die den term exakt definiert
- mit p(X) anfangen, aufteilung in p(x_i)*... ist eine annahme!
- Credit to Berthold Bäuml and Paul Bergmann
- Um p(x|z) zu modeln, müsste der generator x und z als input bekommen und eine zahl als output liefern -> kein generative model!
- Ganz am Schluss, wenn der Text steht, den Post durchgehen und überall wo sinnvoll Grafiken einfügen, bzw eigene Figures generieren (manifold vs non-manifold)
- latent variable z or latent variableS z?

Wenn wir p(x|z) modeln wollen, also z als Zusatzinfo haben, dann wird der manifold zwar simpler (falls z Sinn macht), denn für unwahrscheinliche z ists egal, was wir ausgeben und für wahrscheinliche z müssen wir nur die durch z kodierten bilder "erkennen", indem wir ihnen eine hohe wahrscheinlichkeit zuweisen. allerdings haben wir weiterhin das problem von "fast überall 0 und nur manchmal sehr sehr groß". könnten wir über RBF lösen, was exakt auf generator + gaussian noise raus kommt.
