---
layout: post
title: "Yet another Derivation of the Variational Autoencoder"
disqus_id: "vae-derivation"
category: "general"
html_title: "Yet another Derivation of the Variational Autoencoder"
meta_description: ""
focus_keyword: "VAE"
summary: ""
stylesheets: ""
javascripts: ""
---

<p><i>"How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?"</i> - the Auto-Encoding Variational Bayes paper by Kingma and Welling [1] starts off with several terms that require knowledge of probabilistic modeling. This can be a challenge for people coming from a Deep Learning background. Luckily, there are several good blog posts [TODO] that introduce Variational Autoencoders in a simplified manner. After reading these, I still had some open questions regarding small details that these posts omitted. So, in this blog post I provide <i>yet another</i> derivation of the Variational Autoencoder aimed at readers with open questions. I will start at zero and define the terms used by Kingma and Welling [1] step by step.</p>

<h3>\(p(x|X)\) is interesting</h3>

<p>We have a dataset, \(X\) consisting of \(N\) examples \(x_1, ..., x_n\). Given \(X = \{x_n\}_n^N\), it would be interesting to know \(p(x|X)\). We could use it to predict if a new \(x\) was sampled from the same probability distribution as our dataset. This is helpful for clustering, anomaly detection, data generation and <a href="https://ermongroup.github.io/cs228-notes/preliminaries/applications/">other unsupervised learning tasks.</a></p>

<p>For modeling \(p(x|X)\), we can try to approximate it with a distribution \(p_\theta(x) \approx p(x|X)\). This distribution \(p_\theta(x)\) is parametrized by \(\theta\), which we choose based on \(X\). \(\theta\) could for example be the parameters of a neural network, which outputs \(p_\theta(x)\), i.e. the prediction for the unknown \(p(x|X)\). <!--To find a good approximation of \(p(x|X)\), we could train the neural network using \(X\) and Maximum Likelihood Estimation.--></p>

<h3>\(p(x|X)\) is hard to model</h3>

<p>The problem with \(p(x|X)\) is that it is almost zero almost everywhere for many interesting datasets. For example, the CIFAR-10 dataset contains 60,000 32x32 images, i.e. points in a 1024-dimensional space, the pixel space. Randomly generating an image in this space will likely not produce an image of an object from one of the 10 classes. Following the manifold hypothesis, we can assume that the data points lie on a lower-dimensional manifold. This means that \(p(x|X) \approx 0\) for all \(x\) outside of this manifold. Thus, \(p(x|X)\) has to be very large for \(x\) on the manifold, since \( \int p(x|X) \,d x = 1\). Such a function is very hard to model with a neural network.</p>

<h3>\(p(x|X,z)\) is easier to model</h3>

<p>We can make modeling \(p(x|X)\) easier by incorporating an assumption into the model architecture. The assumption is that the examples \(x\) depend on latent/hidden variables \(z\). That means there are hidden causes behind the examples in our dataset. This is a natural assumption for many datasets. For example, the latent variables that images of faces depend on include azimuth, skin color, lighting etc. While these impact the observable image, we often don't know the exact values of these latent variables - as their name suggests. </p>

<!--TODO: figure with manifold learning josh tenenbaum-->

<p>If we knew the exact values of \(z\) for every \(x\), modeling \(p(x|X, z)\) would likely be easier than modeling \(p(x|X)\) directly. This is evident for simple datasets, where a few latent variables can completely explain every sample. For example, in the figure above, the elevation and azimuth variables completely determine every image. If we can compute the image \(g\) from these two variables \(z\), we can directly model \(p(x|X, z)\) - simply by checking if \(x = g\). But even if there is uncertainty left with respect to \(x\), modeling \(p(x|X, z)\) will likely be easier than modeling \(p(x|X)\) directly. For example, imagine what the probability distribution of handwritten zeros looks like in pixel space. It is easier to model than the probability distribution of handwritten digits in general. There, we are missing the hidden information of what digit is depicted.</p>

<p>Given that we also know the probability distribution \(p(z)\) of the latent variables themselves, we can now model \(p(x|X)\) via</p>

$$p_\theta(x) = \int_z p_\theta(x|z) p_\theta(z) \,dz$$

<p>Here, \(\theta\) consists of all parameters needed to model \(p(x|X,z)\) and \(p(z)\). These indirectly also parametrize \(p_\theta(x)\). In conclusion, knowing the exact values of some latent variables \(z\) for a given \(x\) will probably make modeling \(p(x|X)\) easier.</p>

<h3>Inference</h3>

<p>The problem is that we don't know the values of the latent variables. We don't even know what latent variables there are. That is what makes them latent. We can, however, infer the values of the latent variables for a given \(x\) via Bayes' rule. Using our models \(p_\theta(x|z)\) and \(p_\theta(z)\), we can compute the probability of \(z\) given \(x\) as</p>

$$p_\theta(z|x) = \frac{p_\theta(x|z) p_\theta(z)}{p_\theta(x)}  = \frac{p_\theta(x|z) p_\theta(z)}{\int_z p_\theta(x|z) p_\theta(z) \,dz}$$

<p>Via this equation, we can infer the values of the latent variables \(z\), which makes modelin \(p(x|X, z)\) easier. You might  have noticed the cyclic dependency here: being able to model \(p(x|X,z)\) well requires good estimates of \(z\). These in turn require accurate models \(p_\theta(x|z)\) and \(p_\theta(z)\), i.e. a good model of \(p(x|X,z)\). This cycle requires us to learn our models iteratively. The closer our \(p_\theta(x|z)\) gets to the actual \(p(x|X, z)\) the better our estimates of \(z\) get, which improves \(p_\theta(x|z)\) again. This iterative learning procedure is analogous to the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-maximization algorithm</a>.</p>

<p>In conclusion, we first choose a number of continuous latent variables. Because they are latent, we don't know anything about their actual distribution. Therefore, we can for example set \(p_\theta(z) = \mathcal{N}(\vec{0}, I)\), which does not depend on \(\theta\). This incorporates the strong prior assumption that each latent variable is normally distributed and independent from other latent variables. Since we can compute \(p_\theta(z|x)\) directly from \(p_\theta(x|z)\) and our fixed \(p(z)\), we only need to learn \(p_\theta(x|z)\). To do this, we start with arbitrary \(\theta\), i.e. arbitrary estimates \(p_\theta(z|x)\). At each step, we update \(\theta\) with the objective of maximizing \(p_\theta(X) = p_\theta(x_1) \cdot \ldots cdot p_\theta(x_N)\). This corresponds to Maximum Likelihood Estimation of \(\theta\) based on the current estimates of \(z\). Since improving \(p_\theta(x|z)\) implies improving these estimates, we iteratively optimize \(p_\theta(x|z)\). Finally, we are able to model \(p(x|X)\) via</p>

$$p_\theta(x) = \int_z p_\theta(x|z) p_(z) \,dz$$

<h3>Intractability</h3>

<p>When trying to implement this training procedure, we immediately face a problem: the integrals. In the equation above we see that computing the marginal distibution \(p_\theta(x)\) involves computing an integral over \(z\). Unfortunately, there is no analytical solution for this integral if we use a complex model for \(p_\theta(x|z)\), such as a deep neural network. Instead, we have to find an approximation of the integral's value. One way to do this is Monte Carlo integration, which uses the fact that</p>

$$\int_z p_\theta(x|z) p_(z) \,dz = \mathbb{E}_{z \sim p_(z)}[p_\theta(x|z)] \approx \dfrac{1}{T} \sum_{t=1}^T p_\theta(x|z^{(t)}), z^{(t)} \sim p_(z)$$

This means we can approximate the integral's value by sampling \(z^{(t)} \sim p_(z)\) \(T\) times and taking the average of \(p_\theta(x|z^{(t)})\). While this approximation works in theory, we have to watch out for probability distributions with disjoint support. Consider the approximation of 

$$\int_x q(x) r(x) \,dx$$

<h3>Notes</h3>
<ul>
    <li>The notation used in this post follows the notation of "Deep Learning" by Goodfellow et al. [TODO]</li>
</ul>

------

TODO:
- Neuronale Netze getrennt einführen! Vorher alles NN agnostisch machen.
- die terme des VAE papers (oder beider VAE paper) der reihe nach einführen und jedes mal eine bootstrap alert box hinzufügen, die den term exakt definiert
- mit p(X) anfangen, aufteilung in p(x_i)*... ist eine annahme!
- Credit to Berthold Bäuml and Paul Bergmann
- Um p(x|z) zu modeln, müsste der generator x und z als input bekommen und eine zahl als output liefern -> kein generative model!
- Ganz am Schluss, wenn der Text steht, den Post durchgehen und überall wo sinnvoll Grafiken einfügen, bzw eigene Figures generieren (manifold vs non-manifold)
- latent variable z or latent variableS z?
- genaue entscheidung hinter prior = gaussian ohne lernbare parameter

Wenn wir p(x|z) modeln wollen, also z als Zusatzinfo haben, dann wird der manifold zwar simpler (falls z Sinn macht), denn für unwahrscheinliche z ists egal, was wir ausgeben und für wahrscheinliche z müssen wir nur die durch z kodierten bilder "erkennen", indem wir ihnen eine hohe wahrscheinlichkeit zuweisen. allerdings haben wir weiterhin das problem von "fast überall 0 und nur manchmal sehr sehr groß". könnten wir über RBF lösen, was exakt auf generator + gaussian noise raus kommt.
