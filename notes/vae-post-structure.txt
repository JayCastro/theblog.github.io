p(x|X) is interesting
- data clustering
- sample generation
- anomaly detection

we can try to model it with parameters theta -> p_theta(x) (omitting the dependance on X as in the VAE paper)

p(x|X) is hard to model, because zero almost everywhere and thus almost infinite on the submanifold (assumption: the data lives on a submanifold), for example pictures

introduce latent/hidden variable z. make x depend on z

generative model p_theta(x, z) = p_theta(x | z) p_theta(z) 

so p_theta(x) = \int p_theta(x | z) p_theta(z) dz

we can chose a simple distribution for p_theta(z), for example a unit Gaussian requiring no parameters. the assumption here is that modeling p_theta(x|z) is easier than modeling p_theta(x). the problem with p_theta(x) was that it is zero almost everywhere. this can be handled by p_theta(z), so p_theta(x|z) can be easier, since z can already encode information. -> seems natural, since samples in the input space are often generated from hidden causes -> latent variables can have meaning

now that we have p_theta(x), we can find theta_MLE with Maximum likelihood estimation

theta_MLE = argmax_theta log p_theta(X) = sum ...

problem: p_theta requires computation of the integral -> for p_theta(x|z) modeled by deep network there is no analytical solution.

we could use monte carlo integration to estimate it and sample from p(z) (insert Erwartungswertformel), but this is very inefficient, since p_theta(x|z) is non-zero only in small z-regions. monte carlo integration does work if the two distributions don't overlap, but it is inefficient -> diagram, das für ein bestimmtes x_i die mass bespielsweise darstellt

sampling from the posterior p_theta(z|x) would work, because the posterior focuses on the correct regions -> nochmal diagram, das für ein bestimmtes x_i die mass beispielsweise darstellt

define importance sampling https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf

next problem: posterior is intractable

solution variational inference using approximation q_phi(z|x)

