TODO: Start with p(X) and then mark the step where we can decompose it into p(x_1)... and what the assumption is (probably no global latent variables)

http://kvfrans.com/variational-autoencoders-explained/
Neural Net Perspective:
- You can train a NN to produce a picture of a cat for some arbitrarily chosen latent values and a specific other picture (for example of a dog) for some other values
- Standard autoencoder uses encoder network for learning the latent values
- Right now, we can only generate an image by using a code, which we get by encoding an image -> cannot generate directly using the unit Gaussian
- Force latent values to follow a unit Gaussian distribution in order to move them closer together -> can also generate images directly by sampling from a unit Gaussian
- Add KL divergence between distribution of the latent variable and unit gaussian to loss to let the learning algorithm choose the trade-off between accurate reconstructions and latent variables close to unit Gaussian -> needs reparameterization trick
- (logic error: the latent variable suddenly became stochastic)
- Stochastic latent variables also improve generalization, because it makes the encoder use the latent space efficiently (so that it can increase the standard deviation to 1 to decrease the KL divergence)

https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
Neural Net Perspective:
- Starts with a stochastic encoder and a stochastic decoder (decoder's distribution is Bernoulli, since it is in pixel space)
- VAE adds a regularizer to the reconstruction loss, the KL from the unit gaussian to the encoder's distribution
  -> "keep the representations of each digit sufficiently diverse"
  -> prevents very different representations of images of the same digit
  -> "keep the representations of the same digit close together" (KL term encourages stochasticity (without it the encoder could just output close to zero variance and use codes in the latent space) -> encoder has to map images from the same digit to the same region)
  -> meaningful representations

Probability Model Perspective:
- Use ancestral sampling to sample from p(x, z)
- For inference (infer good latent representations from observed data / calculate the posterior p(z|x)), we can use Bayes
- Denominator makes posterior intractable since we would need to integrate over z
- Approximate posterior by minimizing KL(q(z|x) || p(z|x)) -> still intractable -> maximize ELBO
- ELBO can be interpreted from the neural net perspective as reconstruction loss + regularizer

!!! Mean-field versus amortized inference !!!

