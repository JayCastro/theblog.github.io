---
layout: post
title: "An Interactive Character-Level Language Model"
disqus_id: "char-lm"
pinned: true
github_url: https://github.com/batzner/tensorlm
category: "general"
html_title: "An Interactive Character-Level Language Model built with LSTMs in TensorFlow"
meta_description: "I let an LSTM read texts one character at a time. Find out, what it learned, by feeding it some letters in this interactive post."
focus_keyword: "Language Model"
summary: "I let a neural network read long texts one letter at a time. Its task was to predict the next letter based on those it had seen so far. Over time, it recognized patterns between letters. Find out what it learned by feeding it some letters."
excerpt: "<p>I let a neural network read long texts one letter at a time. Its task was to predict the next letter based on those it had seen so far. Over time, it recognized patterns between letters. Find out what it learned by feeding it some letters below. When you click the send button on the right, it will read your text and auto-complete it.</p>
<p>You can choose between networks that read a lot of Wikipedia articles, US Congress transcripts etc.</p>
<a href=\"/post/character-language-model-lstm-tensorflow\" style=\"color:white\"><div class=\"talk-box\" >
    <table>
        <tr class=\"talk-box-heading\">
            <td></td>
            <td class=\"no-stretch\">
                <div>Generate text from</div>
                <div class=\"btn-group\" role=\"group\" aria-label=\"Dataset choice\">
                    <button type=\"button\" value=\"congress\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">US Congress</button>
                    <button type=\"button\" value=\"wiki\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">Wikipedia</button>
                    <button type=\"button\" value=\"sherlock\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">Sherlock Holmes</button>
                    <button type=\"button\" value=\"southPark\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">South Park</button>
                    <button type=\"button\" value=\"goethe\" onclick=\"selectDataset(this.value)\" class=\"btn btn-default dark\">Goethe</button>
                </div>
            </td>
            <td></td>
        </tr>
        <tr class=\"talk-box-body\">
            <td></td>
            <td>
                <div class=\"talk-box-input\" onclick=\"focusOnInput()\">
                    <div class=\"text-input\" contentEditable=\"true\" oninput=\"onInput()\"></div>
                    <div class=\"text-input-output-bridge\">...</div>
                    <span class=\"text-output\"></span>
                </div>
            </td>
            <td>
                <button type=\"button\" class=\"send-button btn btn-default round btn-icon\" onclick=\"completeText()\">
                    <span class=\"fa fa-paper-plane\"></span>
                </button>
            </td>
        </tr>
    </table>
</div>

<div class=\"talk-box mobile\">
    <div class=\"talk-box-heading\">
        <div>Generate text from</div>
        <div id=\"talk-box-dataset-dropdown\" class=\"btn-group dropdown-full-width\">
            <button type=\"button\" value=\"wiki\" class=\"btn btn-default dropdown-toggle dark\" onchange=\"selectDataset(null)\" data-toggle=\"dropdown\" aria-haspopup=\"true\" aria-expanded=\"false\">
                <span class=\"choice\">Wikipedia</span> <span class=\"caret\"></span>
            </button>
            <ul class=\"dropdown-menu dark\">
                <li><a href=\"#\" data-value=\"congress\">US Congress</a></li>
                <li><a href=\"#\" data-value=\"wiki\">Wikipedia</a></li>
                <li><a href=\"#\" data-value=\"sherlock\">Sherlock Holmes</a></li>
                <li><a href=\"#\" data-value=\"southPark\">South Park</a></li>
                <li><a href=\"#\" data-value=\"goethe\">Goethe Poems</a></li>
            </ul>
        </div>
    </div>
    <div class=\"talk-box-body\">
        <div class=\"talk-box-input\" onclick=\"focusOnInput()\">
            <div class=\"text-input\" contentEditable=\"true\" oninput=\"onInput()\"></div>
            <div class=\"text-input-output-bridge\">...</div>
            <span class=\"text-output\"></span>
        </div>
        <button type=\"button\" class=\"send-button btn btn-default dark\" onclick=\"completeText()\">
            Send
        </button>
    </div>
</div></a>"
tags: [neural-networks, lstm, nlp]
stylesheets: "<link rel=\"stylesheet\" href=\"/css/posts/char-lm.css\">"
javascripts: "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/he/1.1.1/he.min.js\"></script>
<script src=\"/js/posts/char-lm/text-complete.js\"></script>"
---
<p>I let a neural network read long texts one letter at a time. Its task was to predict the next letter based on those it had seen so far. Over time, it recognized patterns between letters. Find out what it learned by feeding it some letters below. When you click the send button on the right / bottom, it will read your text and auto-complete it.</p>
<p>You can choose between networks that read a lot of Wikipedia articles, US Congress transcripts etc.</p>

<div class="talk-box">
    <table>
        <tr class="talk-box-heading">
            <td></td>
            <td class="no-stretch">
                <div>Generate text from</div>
                <div class="btn-group" role="group" aria-label="Dataset choice">
                    <button type="button" value="congress" onclick="selectDataset(this.value)" class="btn btn-default dark">US Congress</button>
                    <button type="button" value="wiki" onclick="selectDataset(this.value)" class="btn btn-default dark">Wikipedia</button>
                    <button type="button" value="sherlock" onclick="selectDataset(this.value)" class="btn btn-default dark">Sherlock Holmes</button>
                    <button type="button" value="southPark" onclick="selectDataset(this.value)" class="btn btn-default dark">South Park</button>
                    <button type="button" value="goethe" onclick="selectDataset(this.value)" class="btn btn-default dark">Goethe</button>
                </div>
            </td>
            <td></td>
        </tr>
        <tr class="talk-box-body">
            <td></td>
            <td>
                <div class="talk-box-input" onclick="focusOnInput()">
                    <div class="text-input" contentEditable="true" oninput="onInput()"></div>
                    <div class="text-input-output-bridge">...</div>
                    <span class="text-output"></span>
                </div>
            </td>
            <td>
                <button type="button" class="send-button btn btn-default round btn-icon" onclick="completeText()">
                    <span class="fa fa-paper-plane"></span>
                </button>
            </td>
        </tr>
    </table>
</div>

<div class="talk-box mobile">
    <div class="talk-box-heading">
        <div>Generate text from</div>
        <div id="talk-box-dataset-dropdown" class="btn-group dropdown-full-width">
            <button type="button" value="wiki" class="btn btn-default dropdown-toggle dark" onchange="selectDataset(null)" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <span class="choice">Wikipedia</span> <span class="caret"></span>
            </button>
            <ul class="dropdown-menu dark">
                <li><a href="#\" data-value="congress">US Congress</a></li>
                <li><a href="#\" data-value="wiki">Wikipedia</a></li>
                <li><a href="#\" data-value="sherlock">Sherlock Holmes</a></li>
                <li><a href="#\" data-value="southPark">South Park</a></li>
                <li><a href="#\" data-value="goethe">Goethe Poems</a></li>
            </ul>
        </div>
    </div>
    <div class="talk-box-body">
        <div class="talk-box-input" onclick="focusOnInput()">
            <div class="text-input" contentEditable="true" oninput="onInput()"></div>
            <div class="text-input-output-bridge">...</div>
            <span class="text-output"></span>
        </div>
        <button type="button" class="send-button btn btn-default dark" onclick="completeText()">
            Send
        </button>
    </div>
</div>

<p>Here is the detailed description of what I did: I used a specific type of recurrent neural networks, the <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">LSTM (Long Short-Term Memory)</a>, to learn a language model for a given text corpus. Because I fed it only one letter at a time, it learned a language model on a character level. This idea is not new at all. It was inspired by Andrej Karpathy's <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank"> blog post</a> about the <i>"Unreasonable Effectiveness of Recurrent Neural Networks"</i>. He also trained character-level networks on Shakespeare, Wikipedia, Linux Source Code etc. The results are amazing.</p>

<p>I decided to try to reproduce his results and make the trained models available via an interactive chat box, so that you can try them out as well.</p>

<h3>Datasets</h3>
<p>These are the datasets I used:</p>
<strong>US Congress</strong>
<ul>
    <li>488 million characters from transcripts of the United States Senate's congressional record</li>
    <li>Trained for 2 days.</li>
</ul>
<strong>Wikipedia</strong>
<ul>
    <li>447 million characters from about 140,000 articles (2.5% of the English Wikipedia)</li>
    <li>Trained for 2 days.</li>
</ul>
<strong>Sherlock</strong>
<ul>
    <li>3.6 million characters (about 650,000 words) from the whole <a href="https://sherlock-holm.es/stories/plain-text/cano.txt" target="_blank">Sherlock Holmes corpus</a> by Sir Arthur Conan Doyle. I removed indentation but kept all line breaks even if their only purpose was formatting.</li>
    <li>Trained for 3 hours.</li>
</ul>
<strong>South Park</strong>
<ul>
    <li>4.7 million characters from all 277 South Park episodes</li>
    <li>Trained for 2 hours.</li>
</ul>
<strong>Goethe</strong>
<ul>
    <li>1.5 million characters from all poems by Johann Wolfgang von Goethe</li>
    <li>Trained for 2 hours.</li>
</ul>
<p>As training / validation split, I used a 90 / 10 ratio for the three small datasets and a 95 / 5 ratio for the US Congress and the Wikipedia dataset.</p>

<h3>Model</h3>
<p>I did not do a lot of hyper-parameter tuning and the tuning I did yielded marginal results. Below are the hyper-parameters that I found to work well for each dataset. For all models, I used 90-dimensional one-hot encodings as input and output of the model. The models were trained by minimizing the cross-entropy / <del>bits</del> nats per character using <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop" target="_blank">RMSprop</a>.</p>

<ul>
    <li>Number of LSTM layers: 3 (the South Park and the Goethe model only had 2 layers)</li>
    <li>Number of neurons per layer: 795</li>
    <li>Batch size: 100</li>
    <li>Learning rate: 0.001</li>
    <li>Dropout: 0.5</li>
    <li>Gradient L2 norm bound: 5</li>
    <li>A dense softmax layer with 90 units followed the last LSTM layer.</li>
    <li>At each training step, the error was backpropagated through 160 time steps / characters.</li>
    <li>I trained all models on an AWS p2.xlarge instance ($0.2 per hour).</li>
    <li>I used early stopping to prevent the model from overfitting.</li>
</ul>

<p>The final models that power the text box above run on an AWS instance. As it turns out, five TensorFlow models with 8 to 13 million parameters each can run simultaneously on a single t2.micro instance with only 1 GiB of RAM.</p>

<h3>Resetting the LSTM state</h3>
<p>I initially trained the LSTM in a stateful manner, meaning that the LSTM's state never gets reset to zero. This allows the model to keep information about the current context beyond the 160 time steps of backpropagation through time. However, I found that this causes the model to generate sentences like this on the Wikipedia dataset:</p>
<blockquote>The Victorian Artist of the Year 1943) was a student of the University of California...</blockquote>
<blockquote>The Victorian Army and the United States Congress of the United States) and the Committee of the American...</blockquote>

<p>Sounds good except for that closing bracket in both sentences. I assume that happens because the model only sees a zero hidden state once during training - at the very beginning. After that, the state never gets reset to a zero vector. Thus, the model can safely store information in the hidden state and even attribute information to zeros in the hidden state, such as "I need to close the bracket soon". For validation and sampling, however, the model starts again with a zero state, so it closes a bracket that was never opened. Resetting the hidden state to zero every now and then solves this problem. I reset the LSTM every 20 training steps, i.e. 3,200 time steps.</p>

<h3>Mutual Perplexity</h3>

<p>I also fed every validation dataset to each of the models and measured their average character <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> (with base e instead of 2).  Here is how confused they were:</p>

<table id="perplexity-table" data-transform="color-cells-by-log-value">
    <tr>
        <td></td>
        <td colspan="5" class="table-heading-columns">Trained on</td>
    </tr>
    <tr>
        <td>Evaluated on</td>
        <th>Sherlock</th>
        <th>Wikipedia</th>
        <th>Congress</th>
        <th>South Park</th>
        <th>Goethe</th>
    </tr>
    <tr>
        <th>Sherlock</th>
        <td>2.94</td>
        <td>8.26</td>
        <td>5.21</td>
        <td>6.25</td>
        <td>183.07</td>
    </tr>
    <tr>
        <th>Wikipedia</th>
        <td>4.35</td>
        <td>3.16</td>
        <td>5.36</td>
        <td>7.31</td>
        <td>186.79</td>
    </tr>
    <tr>
        <th>Congress</th>
        <td>7.39</td>
        <td>3.77</td>
        <td>2.21</td>
        <td>7.68</td>
        <td>270.40</td>
    </tr>
    <tr>
        <th>South Park</th>
        <td>7.10</td>
        <td>4.90</td>
        <td>6.04</td>
        <td>3.35</td>
        <td>206.41</td>
    </tr>
    <tr>
        <th>Goethe</th>
        <td>66.69</td>
        <td>14.43</td>
        <td>35.53</td>
        <td>49.41</td>
        <td>5.36</td>
    </tr>
</table>

<p>We can see that Goethe was quite confused by the English language.</p>

<h3>Code</h3>
<p>I published my code on <a href="https://github.com/batzner/tensorlm">GitHub</a> and as a <a href="https://pypi.python.org/pypi/tensorlm">PyPI package</a> that lets you create your own language model in just a few lines of code:</p>

<pre><code>import tensorflow as tf
from tensorlm import CharLM

with tf.Session() as session:

    # Create a new model. You can also use WordLM
    model = CharLM(session, "datasets/sherlock/tinytrain.txt", max_vocab_size=96,
                   neurons_per_layer=100, num_layers=3, num_timesteps=15)

    # Train it
    model.train(session, max_epochs=10, max_steps=500)

    # Let it generate a text
    generated = model.sample(session, "The ", num_steps=100)
    print("The " + generated)</code></pre>